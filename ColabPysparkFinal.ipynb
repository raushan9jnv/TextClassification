{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+RsE+nviXUT0AsY/aixIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raushan9jnv/TextClassification/blob/main/ColabPysparkFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8KiT0bBBHih"
      },
      "outputs": [],
      "source": [
        "#Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we will install Apache Spark 3.0.1 with Hadoop 2.7\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "yS5Dw1j3CETm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we just need to unzip that folder\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UbuKSICCEQU",
        "outputId": "26fc363b-b6b7-441b-c1b0-b4d298e8b5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.0.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library.\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "ijsLEUbyCRmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "#https://stackoverflow.com/questions/55240940/error-while-installing-spark-on-google-colab"
      ],
      "metadata": {
        "id": "GOGwokWjDh7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "FKXLgLaoCRhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bibPWkyHD0al",
        "outputId": "185aa8d9-39b7-43ee-9c31-fd3a26ba3ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.0.0-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-p7dVKlD0Q8",
        "outputId": "2f02b33d-6f09-440f-8a82-5af38d6805a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=16851206221bbb1ade42335442dfe2d99ded938767ca8ebe1b470c596d1e44fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification"
      ],
      "metadata": {
        "id": "CGX5L9jOGMzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import first\n",
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "from pyspark.sql.functions import mean, countDistinct,desc\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TextClassificationModel\").getOrCreate()\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "        \n",
        "    def show_summary(self):\n",
        "        print(\"DataFrame Schema:\")\n",
        "        self.df.printSchema()\n",
        "        print(\"\\nDataFrame Summary:\")\n",
        "        self.df.describe().show()\n",
        "        print(\"\\nDataFrame Head:\")\n",
        "        self.df.show(5)\n",
        "\n",
        " \n",
        "\n",
        "    def clean_dataset(self):\n",
        "        # perform any necessary cleaning of the dataset here\n",
        "\n",
        "        # Extract categorical and numeric columns\n",
        "        categorical_cols = [col for col in self.df.dtypes if col[1] == \"string\"]\n",
        "        numeric_cols = [col for col in self.df.dtypes if col[1] in [\"double\", \"int\"]]\n",
        "\n",
        "        # Handling null values for categorical columns\n",
        "        for col, _ in categorical_cols:\n",
        "            mode_value = self.df.groupBy().agg(countDistinct(col).alias(\"count\")).sort(desc(\"count\")).limit(1).collect()[0][0]\n",
        "            self.df = self.df.fillna(mode_value, subset=[col])\n",
        "\n",
        "        # Handling null values for numeric columns\n",
        "        for col, _ in numeric_cols:\n",
        "            mean_value = self.df.agg(mean(col)).first()[0]\n",
        "            self.df = self.df.fillna(mean_value, subset=[col])\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def feature_segregation(self):\n",
        "        print(\"feature segregation started\")\n",
        "        if len(self.text_columns) > 1:\n",
        "            self.df = self.df.withColumn(\"text\", concat_ws(\" \", *self.text_columns))\n",
        "        else:\n",
        "            self.df = self.df.withColumnRenamed(self.text_columns[0], \"text\")\n",
        "        self.X = self.df.select(\"text\")\n",
        "        self.y = self.df.select(self.categorical_column)\n",
        "        print(\"feature segregation completed\")\n",
        "\n",
        "    def feature_extraction(self):\n",
        "        print(\"feature extraction started\")\n",
        "        tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\s+\")\n",
        "        self.X = tokenizer.transform(self.X)\n",
        "\n",
        "        hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"tf_features\")\n",
        "        self.X = hashing_tf.transform(self.X)\n",
        "\n",
        "        idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "        self.X = idf.fit(self.X).transform(self.X)\n",
        "        # print(self.X.show())\n",
        "        print(\"feature extraction completed\")\n",
        "\n",
        "    def train_model(self):\n",
        "        self.classes_ = self.y.distinct().collect()\n",
        "\n",
        "        # Create a vector assembler to combine all feature columns into one vector column\n",
        "        assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"vector_features\")\n",
        "        data = assembler.transform(self.X)\n",
        "        # add self.categorical_column with data\n",
        "        # data = data.withColumn(\"Master_SOP\", lit(self.categorical_column))\n",
        "        print(data.show(5))\n",
        "\n",
        "\n",
        "        # Split the data into training and testing datasets\n",
        "        (trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
        "        print(self.categorical_column)\n",
        "        print(self.categorical_column)\n",
        "\n",
        "        self.models = {\n",
        "            # 'Logistic Regression': LogisticRegression(featuresCol=\"vector_features\"),\n",
        "            # 'Naive Bayes': NaiveBayes(featuresCol=\"vector_features\"),\n",
        "            'Decision Tree': DecisionTreeClassifier(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "            # 'Random Forest': RandomForestClassifier(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "            # 'Linear SVC': LinearSVC(featuresCol=\"vector_features\", labelCol=self.categorical_column)\n",
        "        }\n",
        "\n",
        "        self.best_model = None\n",
        "        self.best_accuracy = 0\n",
        "        for name, model in self.models.items():\n",
        "            # Train the model\n",
        "            model = model.fit(trainingData)\n",
        "\n",
        "            # Evaluate the model on test data\n",
        "            predictions = model.transform(testData)\n",
        "            accuracy = predictions.filter(predictions[self.categorical_column] == predictions[\"prediction\"]).count() / predictions.count()\n",
        "\n",
        "            # Save the best model\n",
        "            if accuracy > self.best_accuracy:\n",
        "                self.best_accuracy = accuracy\n",
        "                self.best_model = model\n",
        "        \n",
        "        print(f\"Best Model: {list(self.models.keys())[list(self.models.values()).index(self.best_model)]} with accuracy: {self.best_accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "df_pyspark=spark.read.option('header','true').csv('data.csv',inferSchema=True)\n",
        "text_columns = [\"Ticket_Title\", \"Application\"] # No text columns in this example\n",
        "categorical_column = \"Master_SOP\"\n",
        "model = TextClassificationModel(df_pyspark, text_columns, categorical_column)\n",
        "\n",
        "model.show_summary()\n",
        "model.clean_dataset()\n",
        "model.feature_segregation()\n",
        "model.feature_extraction()\n",
        "model.train_model()\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cTcVuyw1FyIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c0997dc-3a61-4a51-ee19-f5e737a9e2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Schema:\n",
            "root\n",
            " |-- Ticket_Number: string (nullable = true)\n",
            " |-- Priority: string (nullable = true)\n",
            " |-- Ticket_Title: string (nullable = true)\n",
            " |-- Ticket_Description: string (nullable = true)\n",
            " |-- Support_Group: string (nullable = true)\n",
            " |-- Application: string (nullable = true)\n",
            " |-- Resolution_Description: string (nullable = true)\n",
            " |-- Master_SOP: string (nullable = true)\n",
            " |-- EASE: string (nullable = true)\n",
            " |-- extracted_phrase: string (nullable = true)\n",
            " |-- last_updated: string (nullable = true)\n",
            "\n",
            "\n",
            "DataFrame Summary:\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|summary|       Ticket_Number|           Priority|        Ticket_Title|  Ticket_Description|     Support_Group|         Application|Resolution_Description|       Master_SOP|                EASE|    extracted_phrase|        last_updated|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|  count|               26313|               7741|                6705|                6333|              4445|                3681|                  3493|             2942|                2365|                2283|                2249|\n",
            "|   mean|1.2348605121512022E9|1.970388713509404E9| 3.661309140346133E9| 3.907139014303572E7|39154.119318181816|  401985.82352941175|     455515.6666666667|        1360403.4|   525569.1538461539|  488031.28571428574|           6824936.0|\n",
            "| stddev| 4.538974448770992E9|2.16022633731397E10|3.012970420722791...|2.9145979514192754E8|   514440.81450685|  1655225.9601941146|    1762123.4255761537|3041671.063394726|  1892829.6500104389|  1823987.3912741677|                 NaN|\n",
            "|    min| \t1 ms28 (svma5685)\"|                   |                    |                    |           thanks\"|                 120|                      |               40|                  50|                  60| 120 in IS452175 ...|\n",
            "|    max| Preferred langu...|                   |  related to POs ...|➤ Could you pleas...|                  |user confirmed pr...|                      |                 |we have successfu...|your existing baa...|     the mentor team|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "\n",
            "DataFrame Head:\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|Ticket_Number|Priority|        Ticket_Title|Ticket_Description|       Support_Group|    Application|Resolution_Description|    Master_SOP| EASE|extracted_phrase|       last_updated|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|   INC7297575|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7261300|      P5|JDE Incident Request|          Catapult|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7288232|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7297678|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7263453|      P5|JDE Incident Request|  Functional Issue|it.co.ams-jdesupp...|  GRP-25 JDE NA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "feature segregation started\n",
            "feature segregation completed\n",
            "feature extraction started\n",
            "feature extraction completed\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|                text|              tokens|         tf_features|            features|     vector_features|Master_SOP|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|JDE Incident Requ...|[jde, incident, r...|(262144,[62699,95...|(262144,[62699,95...|(262144,[62699,95...|Master_SOP|\n",
            "|JDE Incident Requ...|[jde, incident, r...|(262144,[62699,95...|(262144,[62699,95...|(262144,[62699,95...|Master_SOP|\n",
            "|JDE Incident Requ...|[jde, incident, r...|(262144,[62699,95...|(262144,[62699,95...|(262144,[62699,95...|Master_SOP|\n",
            "|JDE Incident Requ...|[jde, incident, r...|(262144,[62699,95...|(262144,[62699,95...|(262144,[62699,95...|Master_SOP|\n",
            "|JDE Incident Requ...|[jde, incident, r...|(262144,[3386,957...|(262144,[3386,957...|(262144,[3386,957...|Master_SOP|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n",
            "Master_SOP\n",
            "Master_SOP\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-e62062446046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_segregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-e62062446046>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Evaluate the model on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Master_SOP must be of type numeric but was actually of type string."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## read the dataset\n",
        "df_pyspark=spark.read.option('header','true').csv('data.csv',inferSchema=True)\n",
        "print(\"DataFrame Schema:\")\n",
        "df_pyspark.printSchema()\n",
        "print(\"\\nDataFrame Summary:\")\n",
        "df_pyspark.describe().show()\n",
        "print(\"\\nDataFrame Head:\")\n",
        "df_pyspark.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02noY9m_JRAu",
        "outputId": "d2f2df32-7c95-4a8a-f51d-c23fb0a8ee81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Schema:\n",
            "root\n",
            " |-- Ticket_Number: string (nullable = true)\n",
            " |-- Priority: string (nullable = true)\n",
            " |-- Ticket_Title: string (nullable = true)\n",
            " |-- Ticket_Description: string (nullable = true)\n",
            " |-- Support_Group: string (nullable = true)\n",
            " |-- Application: string (nullable = true)\n",
            " |-- Resolution_Description: string (nullable = true)\n",
            " |-- Master_SOP: string (nullable = true)\n",
            " |-- EASE: string (nullable = true)\n",
            " |-- extracted_phrase: string (nullable = true)\n",
            " |-- last_updated: string (nullable = true)\n",
            "\n",
            "\n",
            "DataFrame Summary:\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|summary|       Ticket_Number|           Priority|        Ticket_Title|  Ticket_Description|     Support_Group|         Application|Resolution_Description|       Master_SOP|                EASE|    extracted_phrase|        last_updated|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|  count|               26313|               7741|                6705|                6333|              4445|                3681|                  3493|             2942|                2365|                2283|                2249|\n",
            "|   mean|1.2348605121512022E9|1.970388713509404E9| 3.661309140346133E9| 3.907139014303572E7|39154.119318181816|  401985.82352941175|     455515.6666666667|        1360403.4|   525569.1538461539|  488031.28571428574|           6824936.0|\n",
            "| stddev| 4.538974448770992E9|2.16022633731397E10|3.012970420722791...|2.9145979514192754E8|   514440.81450685|  1655225.9601941146|    1762123.4255761537|3041671.063394726|  1892829.6500104389|  1823987.3912741677|                 NaN|\n",
            "|    min| \t1 ms28 (svma5685)\"|                   |                    |                    |           thanks\"|                 120|                      |               40|                  50|                  60| 120 in IS452175 ...|\n",
            "|    max| Preferred langu...|                   |  related to POs ...|➤ Could you pleas...|                  |user confirmed pr...|                      |                 |we have successfu...|your existing baa...|     the mentor team|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "\n",
            "DataFrame Head:\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|Ticket_Number|Priority|        Ticket_Title|Ticket_Description|       Support_Group|    Application|Resolution_Description|    Master_SOP| EASE|extracted_phrase|       last_updated|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|   INC7297575|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7261300|      P5|JDE Incident Request|          Catapult|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7288232|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7297678|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7263453|      P5|JDE Incident Request|  Functional Issue|it.co.ams-jdesupp...|  GRP-25 JDE NA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvUklGYZOEmA",
        "outputId": "64003e8f-0afd-4ae4-9009-9c43f2bc2577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Ticket_Number: string, Priority: string, Ticket_Title: string, Ticket_Description: string, Support_Group: string, Application: string, Resolution_Description: string, Master_SOP: string, EASE: string, extracted_phrase: string, last_updated: string]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# # Create a sample DataFrame\n",
        "# df = pd.DataFrame({\n",
        "#     \"name\": [\"John\", \"Jane\", \"Jim\", \"Jessica\", \"Jack\"],\n",
        "#     \"age\": [31, 32, np.nan, 35, np.nan],\n",
        "#     \"gender\": [\"Male\", \"Female\", \"Male\", \"Female\", \"Male\"],\n",
        "#     \"income\": [50000, 55000, np.nan, 60000, np.nan]\n",
        "# })\n",
        "\n",
        "# # Initialize the TextClassificationModel\n",
        "text_columns = [] # No text columns in this example\n",
        "categorical_column = []\n",
        "model = TextClassificationModel(df_pyspark, text_columns, categorical_column)\n",
        "\n",
        "# Show the summary of the DataFrame\n",
        "# model.show_summary()\n",
        "\n",
        "# Clean the DataFrame\n",
        "model.clean_dataset()\n",
        "\n",
        "# Show the summary of the cleaned DataFrame\n",
        "model.show_summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUkbKu5rGg84",
        "outputId": "88625be7-0ae1-44a0-89ae-f2f733224f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Schema:\n",
            "root\n",
            " |-- Ticket_Number: string (nullable = true)\n",
            " |-- Priority: string (nullable = true)\n",
            " |-- Ticket_Title: string (nullable = true)\n",
            " |-- Ticket_Description: string (nullable = true)\n",
            " |-- Support_Group: string (nullable = true)\n",
            " |-- Application: string (nullable = true)\n",
            " |-- Resolution_Description: string (nullable = true)\n",
            " |-- Master_SOP: string (nullable = true)\n",
            " |-- EASE: string (nullable = true)\n",
            " |-- extracted_phrase: string (nullable = true)\n",
            " |-- last_updated: string (nullable = true)\n",
            "\n",
            "\n",
            "DataFrame Summary:\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|summary|       Ticket_Number|           Priority|        Ticket_Title|  Ticket_Description|     Support_Group|         Application|Resolution_Description|       Master_SOP|                EASE|    extracted_phrase|        last_updated|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|  count|               26313|               7741|                6705|                6333|              4445|                3681|                  3493|             2942|                2365|                2283|                2249|\n",
            "|   mean|1.2348605121512022E9|1.970388713509404E9| 3.661309140346133E9| 3.907139014303572E7|39154.119318181816|  401985.82352941175|     455515.6666666667|        1360403.4|   525569.1538461539|  488031.28571428574|           6824936.0|\n",
            "| stddev| 4.538974448770992E9|2.16022633731397E10|3.012970420722791...|2.9145979514192754E8|   514440.81450685|  1655225.9601941146|    1762123.4255761537|3041671.063394726|  1892829.6500104389|  1823987.3912741677|                 NaN|\n",
            "|    min| \t1 ms28 (svma5685)\"|                   |                    |                    |           thanks\"|                 120|                      |               40|                  50|                  60| 120 in IS452175 ...|\n",
            "|    max| Preferred langu...|                   |  related to POs ...|➤ Could you pleas...|                  |user confirmed pr...|                      |                 |we have successfu...|your existing baa...|     the mentor team|\n",
            "+-------+--------------------+-------------------+--------------------+--------------------+------------------+--------------------+----------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "\n",
            "DataFrame Head:\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|Ticket_Number|Priority|        Ticket_Title|Ticket_Description|       Support_Group|    Application|Resolution_Description|    Master_SOP| EASE|extracted_phrase|       last_updated|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "|   INC7297575|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7261300|      P5|JDE Incident Request|          Catapult|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7288232|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7297678|      P5|JDE Incident Request|             Other|it.co.ams-jdesupp...|GRP-25 JDE EMEA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "|   INC7263453|      P5|JDE Incident Request|  Functional Issue|it.co.ams-jdesupp...|  GRP-25 JDE NA|                 Other|Not Applicable|Other|           Other|2023-01-11 08:14:38|\n",
            "+-------------+--------+--------------------+------------------+--------------------+---------------+----------------------+--------------+-----+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, mode\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        # perform any necessary cleaning of the dataset here\n",
        "\n",
        "        # Extract categorical and numeric columns\n",
        "        categorical_cols = [col for col in self.df.dtypes if col[1] == \"string\"]\n",
        "        numeric_cols = [col for col in self.df.dtypes if col[1] in [\"double\", \"int\"]]\n",
        "\n",
        "        # Handling null values for categorical columns\n",
        "        for col, _ in categorical_cols:\n",
        "            self.df = self.df.fillna(self.df.groupBy().agg(mode(col).alias(col)), subset=[col])\n",
        "\n",
        "        # Handling null values for numeric columns\n",
        "        for col, _ in numeric_cols:\n",
        "            self.df = self.df.fillna(self.df.agg(mean(col)).first()[0], subset=[col])\n",
        "\n",
        "        return self.df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9vEEnEypSHkM",
        "outputId": "32f620d7-714b-4b89-d032-7b7ca147ba64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3f121be87f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextClassificationModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'mode' from 'pyspark.sql.functions' (/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, countDistinct,desc\n",
        "\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        # perform any necessary cleaning of the dataset here\n",
        "\n",
        "        # Extract categorical and numeric columns\n",
        "        categorical_cols = [col for col in self.df.dtypes if col[1] == \"string\"]\n",
        "        numeric_cols = [col for col in self.df.dtypes if col[1] in [\"double\", \"int\"]]\n",
        "\n",
        "        # Handling null values for categorical columns\n",
        "        for col, _ in categorical_cols:\n",
        "            mode_value = self.df.groupBy().agg(countDistinct(col)).sort(desc(\"count(DISTINCT %s)\" % col)).limit(1).collect()[0][0]\n",
        "            self.df = self.df.fillna(mode_value, subset=[col])\n",
        "\n",
        "        # Handling null values for numeric columns\n",
        "        for col, _ in numeric_cols:\n",
        "            mean_value = self.df.agg(mean(col)).first()[0]\n",
        "            self.df = self.df.fillna(mean_value, subset=[col])\n",
        "\n",
        "        return self.df\n"
      ],
      "metadata": {
        "id": "nm-ojjf-SiAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, countDistinct, desc\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        # perform any necessary cleaning of the dataset here\n",
        "\n",
        "        # Extract categorical and numeric columns\n",
        "        categorical_cols = [col for col in self.df.dtypes if col[1] == \"string\"]\n",
        "        numeric_cols = [col for col in self.df.dtypes if col[1] in [\"double\", \"int\"]]\n",
        "\n",
        "        # Handling null values for categorical columns\n",
        "        for col, _ in categorical_cols:\n",
        "            mode_value = self.df.groupBy().agg(countDistinct(col).alias(\"count\")).sort(desc(\"count\")).limit(1).collect()[0][0]\n",
        "            self.df = self.df.fillna(mode_value, subset=[col])\n",
        "\n",
        "        # Handling null values for numeric columns\n",
        "        for col, _ in numeric_cols:\n",
        "            mean_value = self.df.agg(mean(col)).first()[0]\n",
        "            self.df = self.df.fillna(mean_value, subset=[col])\n",
        "\n",
        "        return self.df\n"
      ],
      "metadata": {
        "id": "TMkOglhETaA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "\n",
        "    def feature_segregation(self):\n",
        "        from pyspark.sql.functions import concat_ws\n",
        "        \n",
        "        if len(self.text_columns) > 1:\n",
        "            self.df = self.df.withColumn(\"text\", concat_ws(\" \", *self.text_columns))\n",
        "        else:\n",
        "            self.df = self.df.withColumnRenamed(self.text_columns[0], \"text\")\n",
        "        self.X = self.df.select(\"text\")\n",
        "        print(self.X.show())\n",
        "        self.y = self.df.select(self.categorical_column)\n",
        "        print(self.y.show())\n",
        "\n",
        "    def feature_extraction(self):\n",
        "        hashing_tf = HashingTF(inputCol=\"text\", outputCol=\"tf_features\")\n",
        "        self.X = hashing_tf.transform(self.X)\n",
        "        idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "        self.X = idf.fit(self.X).transform(self.X)\n",
        "\n",
        "\n",
        "text_columns = [\"Ticket_Title\", \"Application\"] # No text columns in this example\n",
        "categorical_column = [\"Master_SOP\"]\n",
        "model = TextClassificationModel(df_pyspark, text_columns, categorical_column)\n",
        "model.feature_segregation()\n",
        "model.feature_extraction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EIuHYBd2Ulw-",
        "outputId": "f0705b2b-10b2-48bc-ffe6-f520bfcfb83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE - Report/Data...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|JDE Incident Requ...|\n",
            "|Need access to mo...|\n",
            "|SIR - Incident ti...|\n",
            "|open a ticket GRP...|\n",
            "|JDE Incident Requ...|\n",
            "|Function Key F2 k...|\n",
            "|JDE Incident Requ...|\n",
            "|issue with JDE GR...|\n",
            "|JDE Incident Requ...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n",
            "+--------------+\n",
            "|    Master_SOP|\n",
            "+--------------+\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "|Not Applicable|\n",
            "+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-afe7a0b7dd5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_segregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-afe7a0b7dd5e>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mhashing_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashing_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf_features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The input column must be array, but got string."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, dataframe, text_columns, categorical_column):\n",
        "        self.df = dataframe\n",
        "        self.text_columns = text_columns\n",
        "        self.categorical_column = categorical_column\n",
        "\n",
        "    def feature_segregation(self):\n",
        "        if len(self.text_columns) > 1:\n",
        "            self.df = self.df.withColumn(\"text\", concat_ws(\" \", *self.text_columns))\n",
        "        else:\n",
        "            self.df = self.df.withColumnRenamed(self.text_columns[0], \"text\")\n",
        "        self.X = self.df.select(\"text\")\n",
        "        self.y = self.df.select(self.categorical_column)\n",
        "\n",
        "    def feature_extraction(self):\n",
        "        tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\s+\")\n",
        "        self.X = tokenizer.transform(self.X)\n",
        "\n",
        "        hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"tf_features\")\n",
        "        self.X = hashing_tf.transform(self.X)\n",
        "\n",
        "        idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "        self.X = idf.fit(self.X).transform(self.X)\n",
        "    \n",
        "    def train_model(self):\n",
        "        unique_classes = self.y.select(F.unique(\"y\").alias(\"classes_\"))\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.X.randomSplit([0.8, 0.2], seed=123)\n",
        "        \n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(),\n",
        "            'Naive Bayes': MultinomialNB(),\n",
        "            'Decision Tree': DecisionTreeClassifier(),\n",
        "            'Random Forest': RandomForestClassifier(),\n",
        "            'Linear SVC': LinearSVC(),\n",
        "            'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(),\n",
        "            'AdaBoost': AdaBoostClassifier()\n",
        "        }\n",
        "        \n",
        "        self.best_model = None\n",
        "        self.best_accuracy = 0\n",
        "        for name, model in models.items():\n",
        "            vec_assembler = VectorAssembler(inputCols=self.X_train.columns, outputCol=\"features\")\n",
        "            pipeline = Pipeline(stages=[vec_assembler, model])\n",
        "            pipeline_model = pipeline.fit(self.X_train)\n",
        "            predictions = pipeline_model.transform(self.X_test)\n",
        "            evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "            accuracy = evaluator.evaluate(predictions)\n",
        "            evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "            precision = evaluator.evaluate(predictions)\n",
        "            evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "            recall = evaluator.evaluate(predictions)\n",
        "            evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"f1\")\n",
        "            f1 = evaluator.evaluate(predictions)\n",
        "            print(f'{name}\\n')\n",
        "            print(f'{name} accuracy: {accuracy}')\n",
        "            print(f'{name} precision: {precision}')\n",
        "            print(f'{name} recall: {recall}')\n",
        "            print(f'{name} f1 score: {f1}')\n",
        "            print(\"-------------------------------------\")\n",
        "            if accuracy > self.best_accuracy:\n",
        "                self.best_accuracy = accuracy\n",
        "                self.best_model = pipeline_model\n",
        "        print(self.best_model)\n",
        "\n",
        "\n",
        "\n",
        "text_columns = [\"Ticket_Title\", \"Application\"] # No text columns in this example\n",
        "categorical_column = [\"Master_SOP\"]\n",
        "model = TextClassificationModel(df_pyspark, text_columns, categorical_column)\n",
        "model.feature_segregation()\n",
        "model.feature_extraction()\n",
        "model.train_model()"
      ],
      "metadata": {
        "id": "OngdhIDgUvVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, MultinomialNB, DecisionTreeClassifier, RandomForestClassifier, LinearSVC, KNeighborsClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TrainModel\").getOrCreate()\n",
        "\n",
        "def train_model(self):\n",
        "    unique_classes = self.y.select(F.unique(\"y\").alias(\"classes_\"))\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = self.X.randomSplit([0.8, 0.2], seed=123)\n",
        "    \n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(),\n",
        "        'Naive Bayes': MultinomialNB(),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'Linear SVC': LinearSVC(),\n",
        "        'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(),\n",
        "        'AdaBoost': AdaBoostClassifier()\n",
        "    }\n",
        "    \n",
        "    self.best_model = None\n",
        "    self.best_accuracy = 0\n",
        "    for name, model in models.items():\n",
        "        vec_assembler = VectorAssembler(inputCols=self.X_train.columns, outputCol=\"features\")\n",
        "        pipeline = Pipeline(stages=[vec_assembler, model])\n",
        "        pipeline_model = pipeline.fit(self.X_train)\n",
        "        predictions = pipeline_model.transform(self.X_test)\n",
        "        evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "        accuracy = evaluator.evaluate(predictions)\n",
        "        evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "        precision = evaluator.evaluate(predictions)\n",
        "        evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "        recall = evaluator.evaluate(predictions)\n",
        "        evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"f1\")\n",
        "        f1 = evaluator.evaluate(predictions)\n",
        "        print(f'{name}\\n')\n",
        "        print(f'{name} accuracy: {accuracy}')\n",
        "        print(f'{name} precision: {precision}')\n",
        "        print(f'{name} recall: {recall}')\n",
        "        print(f'{name} f1 score: {f1}')\n",
        "        print(\"-------------------------------------\")\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.best_model = pipeline_model\n",
        "    print(self.best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "2OsA0uUPjDfT",
        "outputId": "a33f536e-88dd-4484-e0f9-f53a3cd1b1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4313d1c3fca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'MultinomialNB' from 'pyspark.ml.classification' (/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/classification.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier, LinearSVC\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "def train_model(self):\n",
        "    # Convert categorical column to integer type\n",
        "    self.y = self.y.withColumn(self.categorical_column, self.y[self.categorical_column].cast(IntegerType()))\n",
        "\n",
        "    # Create a vector assembler to combine all feature columns into one vector column\n",
        "    assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features\")\n",
        "    data = assembler.transform(self.X.join(self.y, on=\"text\"))\n",
        "\n",
        "    # Split the data into training and testing datasets\n",
        "    (trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
        "\n",
        "    # Initialize different classifiers\n",
        "    self.models = {\n",
        "        'Logistic Regression': LogisticRegression(featuresCol=\"features\", labelCol=self.categorical_column),\n",
        "        'Naive Bayes': NaiveBayes(featuresCol=\"features\", labelCol=self.categorical_column),\n",
        "        'Decision Tree': DecisionTreeClassifier(featuresCol=\"features\", labelCol=self.categorical_column),\n",
        "        'Random Forest': RandomForestClassifier(featuresCol=\"features\", labelCol=self.categorical_column),\n",
        "        'Linear SVC': LinearSVC(featuresCol=\"features\", labelCol=self.categorical_column)\n",
        "    }\n",
        "\n",
        "    self.best_model = None\n",
        "    self.best_accuracy = 0\n",
        "    for name, model in self.models.items():\n",
        "        # Train the model\n",
        "        classifier = model.fit(trainingData)\n",
        "\n",
        "        # Evaluate the model on test data\n",
        "        predictions = classifier.transform(testData)\n",
        "        evaluator = MulticlassClassificationEvaluator(labelCol=self.categorical_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "        accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "        # Choose the best model based on accuracy\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.best_model = classifier\n",
        "        \n",
        "    print(\"Best Model: \", list(self.models.keys())[list(self.models.values()).index(self.best_model)])\n",
        "    print(\"Accuracy: \", self.best_accuracy)\n"
      ],
      "metadata": {
        "id": "vbyGTkh5siAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier, LinearSVC\n",
        "\n",
        "def train_model(self):\n",
        "    self.classes_ = self.y.distinct().collect()\n",
        "\n",
        "    # Create a vector assembler to combine all feature columns into one vector column\n",
        "    assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"vector_features\")\n",
        "    data = assembler.transform(self.X)\n",
        "\n",
        "    # Split the data into training and testing datasets\n",
        "    (trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
        "\n",
        "    self.models = {\n",
        "        'Logistic Regression': LogisticRegression(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "        'Naive Bayes': NaiveBayes(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "        'Decision Tree': DecisionTreeClassifier(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "        'Random Forest': RandomForestClassifier(featuresCol=\"vector_features\", labelCol=self.categorical_column),\n",
        "        'Linear SVC': LinearSVC(featuresCol=\"vector_features\", labelCol=self.categorical_column)\n",
        "    }\n",
        "\n",
        "    self.best_model = None\n",
        "    self.best_accuracy = 0\n",
        "    for name, model in self.models.items():\n",
        "        # Train the model\n",
        "        model = model.fit(trainingData)\n",
        "\n",
        "        # Evaluate the model on test data\n",
        "        predictions = model.transform(testData)\n",
        "        accuracy = predictions.filter(predictions[self.categorical_column] == predictions[\"prediction\"]).count() / predictions.count()\n",
        "\n",
        "        # Save the best model\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.best_model = model\n",
        "    \n",
        "    print(f\"Best Model: {list(self.models.keys())[list(self.models.values()).index(self.best_model)]} with accuracy: {self.best_accuracy}\")\n"
      ],
      "metadata": {
        "id": "y5ORC43TueHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}